### Todo

#### Priority 0 (smaller is more important)
+ move this into github's tracking system 

+ refactor webcrawler code into separate library
  * refactor/rename all packages to something more specific from "biz"
  * separate Build.scala into two separate Build.scala files (one for dead link finding
    specific code, another for webcrawler related code).

+ Define API interface / json format for chunks to push to client

+ Implement API interface

+ fix invalid API URLs to server so they dont return html

+ update scala-async to the latest compatible version

+ modularize existing CheckUrlCrawlability code, using stackable traits pattern?

#### Priority 1

+ Add sitemaps to initial crawling

+ Needs testing: Figure out how to only download x kb from websites (truncate pages) to avoid crawling pages that are huge -- solution, use spray-client. if over limit, returns an error instead

+ Needs testing: make sure correct exceptions are serialized in certain circumstances

+ Add more persistence options for visited url storage, add option for 
+ global state, redis, etc

+ Add bloom filter implementation for visited urls

+ Add Dependency Injection in visited urls algorithm

+ Add support for parsing <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW, NONE">
http://googlewebmastercentral.blogspot.com/2007/03/using-robots-meta-tag.html

#### Priority 3
+ fix compiler warnings


### Notes

+ So yea.. don't shut down Akka.system or restart it, the code for throttling HTTP requests
  relies on it being up, since the throttling mechanism uses akka actors.

### Build Status
[![Build Status](https://travis-ci.org/daniel-trinh/webcrawler.png?branch=wip)](https://travis-ci.org/daniel-trinh/webcrawler)